{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12\n",
    "\n",
    "Neural Networks for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/image_utils.py\n",
    "\n",
    "!wget -qO- https://github.com/DM-GY-9103-2024F-H/9103-utils/releases/latest/download/lfw.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "\n",
    "from WK12_utils import LFWUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images\n",
    "\n",
    "Can we use NN models to do classification of images? Sure.\n",
    "\n",
    "The steps are the same, we just have to load image data and adapt the cost/loss function to calculate some kind of classification metric instead.\n",
    "\n",
    "We'll use the _Labeled Faces in the Wild_ dataset from last homework.\n",
    "\n",
    "The steps for setting up the classification model will be:\n",
    "\n",
    "- Load dataset and do any kind of pre-processing\n",
    "- Split data into train/test datasets\n",
    "- Split independent features and classification label and load them into `Tensors`\n",
    "- Create `DataLoader` instances (we'll see what this means below)\n",
    "- Build a NN model\n",
    "- Set up an optimizer\n",
    "- Pick a cost/loss function\n",
    "- Implement an evaluation function and any other kind of visualization that helps quantify the model\n",
    "- Train model\n",
    "- Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split Dataset\n",
    "\n",
    "The `LFWUtils.train_test_split(0.5)` function gives us some `Python` objects we can use to create our `Tensor`s.\n",
    "\n",
    "The `pixels` key gives us a list of the images' pixel data, and the `label` key gives us the images' label IDs.\n",
    "\n",
    "We don't have to do any normalization since the pixels will be in a know, well-defined range of $[0 \\text{ - } 255]$.\n",
    "\n",
    "The only thing we have to do differently is cast the labels `Tensor` to `long`. This is to ensure the numbers in those `Tensor`s are whole numbers and don't have decimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5)\n",
    "\n",
    "x_train = Tensor(train[\"pixels\"])\n",
    "y_train = Tensor(train[\"labels\"]).long()\n",
    "\n",
    "x_test = Tensor(test[\"pixels\"])\n",
    "y_test = Tensor(test[\"labels\"]).long()\n",
    "\n",
    "print(\"Dataset Samples\")\n",
    "print(\"\\tTrain:\", len(x_train))\n",
    "print(\"\\tTest:\", len(x_test))\n",
    "\n",
    "print(\"\\nDataset Shape:\", list(x_train.shape))\n",
    "print(\"\\nSample Shape:\", list(x_train[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at data\n",
    "\n",
    "We can visualize some of the images, their text labels and label IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, len(train[\"pixels\"]), 100):\n",
    "  id = train[\"labels\"][idx]\n",
    "  print(id, LFWUtils.LABELS[id])\n",
    "  display(make_image(train[\"pixels\"][idx], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets & DataLoaders\n",
    "\n",
    "This is new !\n",
    "\n",
    "We could try to train this neural network exactly how we trained the previous one where we gave the model the entire training dataset at once and asked for it to minimize the cost function over all of the samples at the same time.\n",
    "\n",
    "This could work for this dataset, but once we start working with bigger and bigger datasets, it will be difficult to ask the computer to perform this kind of optimization over all of the images at the same time.\n",
    "\n",
    "We have to split up our dataset into batches, and ask the model to work on subsets of our datasets. Since we're not giving the model all of the data at once, we should also randomize the order of the data to make sure the order in which the model sees a sample doesn't affect its influence on the overall quality of the model.\n",
    "\n",
    "We'll use `PyTorch`'s built in [Datasets and DataLoaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) classes to help us manage our batches of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Dataset Class\n",
    "\n",
    "The first thing we have to do is define a `PyTorch` `Dataset` class that will handle our pixel and label data.\n",
    "\n",
    "This class need to have $3$ functions defined:\n",
    "\n",
    "`__init__()`: the constructor. Should receive all the info from a dataset.\n",
    "\n",
    "`__len__()`: this returns how many items we have in out dataset.\n",
    "\n",
    "`__getitem__()`: given an index, return the corresponding pixels and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(Dataset):\n",
    "  def __init__(self, imgs, labels):\n",
    "    self.imgs = imgs\n",
    "    self.labels = labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return [self.imgs[idx], self.labels[idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "\n",
    "Now we can just create `Dataset` instances for each of our $2$ datasets, and pass those along to the `DataLoader` constructor.\n",
    "\n",
    "In the `DataLoader` constructor we can use the `batch_size` parameter to define how many images the model should consider each time it does gradient descent, and the `shuffle` parameter to specify whether the data should be randomized during that process.\n",
    "\n",
    "Setting parameters for the training `DataLoader` is more important since the batch size and randomization will directly affect the quality of the model.\n",
    "\n",
    "For the test `DataLoader` the batch size won't really make a difference in the results, although larger batches should evaluate faster, and shuffling might confuse us when we try to look at specific test cases that are failing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(FaceDataset(x_train, y_train), batch_size=128, shuffle=True)\n",
    "test_dataloader = DataLoader(FaceDataset(x_test, y_test), batch_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check `DataLoader`\n",
    "\n",
    "When we iterate through a `DataLoader` with a for loop, we get a sub-section of our dataset called a `batch`.\n",
    "\n",
    "Since the `__getitem__()` function of our `FaceDataset` class returns a list of $2$ things (pixels and a label), the `batch` in our `DataLoader` will also have collections of those $2$ things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "  print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, labels in test_dataloader:\n",
    "  print(imgs.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Optimizer and Cost Function\n",
    "\n",
    "We'll start with the simplest kind of network again, with just an input and an output layer.\n",
    "\n",
    "The input layer has as many neurons as the number of pixels in each image, and the output layer has one neuron per possible class.\n",
    "\n",
    "It looks like this, and is juts like our regression network above, but has more output neurons:\n",
    "\n",
    "<img src=\"./imgs/linear_22100x26.jpg\" width=\"800px\"/>\n",
    "\n",
    "Our optimizer will be `SGD` again. Depending on the dataset and model being created, `SGD` can perform even better with batched inputs because it is looking at less data and is less constrained within each batch.\n",
    "\n",
    "Our cost function is a bit different. Previously, we used $L2$ distances to calculate the root mean square error of our regression predictions and used that value as the cost function for gradient descent.\n",
    "\n",
    "In order to use gradient descent for classification, we have to turn the discrete nature of our labels/classes and their errors into something that has smooth and integratable slopes.\n",
    "\n",
    "That's what the `CrossEntropyLoss()` function does for us. It looks at the outputs of our model and transforms the regression-type continuous values at our outputs into class prediction probabilities in a way that gradient descent still works.\n",
    "\n",
    "There's more information in the [`PyTorch` documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(x_train.shape[1], len(y_train.unique()))\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n",
    "\n",
    "We can train the model now.\n",
    "\n",
    "We still don't have an evaluation function, but we can use the values from the loss function to adjust parameters and make sure that the model is learning.\n",
    "\n",
    "We'll train for $32$ epochs, and in each epoch we have to iterate through the data that is inside our `DataLoader` object.\n",
    "\n",
    "The `x` and `y` variables below actually hold pixel and label information for $256$ images. For each of these batches we predict labels, calculate loss, calculate the slope of the loss function, update model parameters, zero the gradients, and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  for imgs, labels in train_dataloader:\n",
    "    # clear slopes in optimizer\n",
    "    optim.zero_grad()\n",
    "\n",
    "    # compute labels from model\n",
    "    labels_pred = model(imgs)\n",
    "\n",
    "    # calculate how wrong model is\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "\n",
    "    # compute slopes for cost function\n",
    "    loss.backward()\n",
    "\n",
    "    # adjust model's parameters\n",
    "    optim.step()\n",
    "\n",
    "  # keep an eye on loss as we train\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The loss/cost value can oscillate up and down a bit, but, overall should steadily decrease.\n",
    "\n",
    "This up and down has to do with the batching and shuffling of our training data. The `SGD` optimizer makes some decisions that it sometimes has to undo, but overall, the model looks like it's learning.\n",
    "\n",
    "We can keep running this cell until the loss gets really small, but what we should do next is think of a way to evaluate our model using the test dataset and a function that gives us something a little more legible than the `CrossEntropyLoss()` value which is the sum of the \"negative log likelihood\" of our predictions.\n",
    "\n",
    "This evaluation function is helpful not only when measuring the overall quality of our model, but should help us detect if/when the model starts to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Function\n",
    "\n",
    "Our `data_utils` file has a `classification_error()` function that calculates a percentage of mistakes between two lists of labels. We just have to give it a list of true labels and a list of predicted labels.\n",
    "\n",
    "Having these lists of labels will also be useful if we want to visualize our predictions in a confusion matrix, so let's write a helper function that takes a model and a `DataLoader` and computes predictions for all of the samples in that dataloader.\n",
    "\n",
    "We'll make sure our model isn't computing gradients with `torch.no_grad()` and also turn off some other features of the model that don't have to run during evaluation with `model.eval()`.\n",
    "\n",
    "The `argmax(dim=1)` function gives us the index of our output neuron with the largest value. This is how we pick a class label from the raw regression-like numbers.\n",
    "\n",
    "Then we append a list of labels to our overall list of prediction and true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(model, dataloader):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    data_labels = []\n",
    "    pred_labels = []\n",
    "    for imgs, labels in dataloader:\n",
    "      labels_pred = model(imgs).argmax(dim=1)\n",
    "      data_labels += [l.item() for l in labels]\n",
    "      pred_labels += [l.item() for l in labels_pred]\n",
    "    return data_labels, pred_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model\n",
    "\n",
    "We can now run the evaluation function on the model and both `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "\n",
    "print(\"train error:\", f\"{classification_error(train_labels, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(test_labels, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Train with Evaluation\n",
    "\n",
    "To make sure our model didn't overfit the training data, we should keep an eye on the evaluation function during training.\n",
    "\n",
    "Let's re-initialize our model and optimizer and re-train our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(x_train.shape[1], len(y_train.unique()))\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  for imgs, labels in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "    test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "    train_error = classification_error(train_labels, train_predictions)\n",
    "    test_error = classification_error(test_labels, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "\n",
    "print(\"train error:\", f\"{classification_error(train_labels, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(test_labels, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Is it overfitting ? Can we keep running the training cell ?\n",
    "\n",
    "How low can we get our test error ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Layers\n",
    "\n",
    "Maybe we can improve our classification by adding some hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model =  nn.Sequential(\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 16),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 16, x_train.shape[1] // 32),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1] // 32, len(y_train.unique())),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  for imgs, labels in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "    test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "    train_error = classification_error(train_labels, train_predictions)\n",
    "    test_error = classification_error(test_labels, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "\n",
    "print(\"train error:\", f\"{classification_error(train_labels, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(test_labels, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The result is mostly the same, which is not surprising.\n",
    "\n",
    "We did add layers, but the network didn't need any extra neurons to do well on the training data.\n",
    "\n",
    "It needs help with the testing data, or, another way to say this is: it needs help generalizing without memorizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make It Harder\n",
    "\n",
    "Neural network models can seem simple to explain in a general sense: they're long and wide computation graphs made up of simple operations that have been tuned to achieve a specific task. Once they're training, or trained, their details and specificities are a little less easy to describe. It's hard to know exactly what each neuron is doing, and what part of the computation they are responsible for. We can train the same network, with the same parameters, using the same input data, and end up with wildly different results.\n",
    "\n",
    "This is one reason why it's hard to debug a network when it doesn't seem to be learning properly, or when it starts to overfit and memorize the training data. Which neurons do we tune ?\n",
    "\n",
    "One common situation that can lead to overfitting is when a network ends up with parameters that make it perform well on the training data without really activating all of its neurons. This is usually what is happening if adding layers to a network doesn't improve its performance.\n",
    "\n",
    "One set of strategies for improving neural network training in these cases involves making the training process harder than it has to be. It's like we're challenging the neural network to learn more than it has, so that later it has an easier time with the regular data.\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "One simple technique to achieve this is to add `Dropout` layers to our network. A `Dropout` layer is a layer of neurons that don't perform any mathematical operation, but are selectively dropped out of the network randomly during training. This has the effect of randomly changing the network's architecture during training and preventing the network from becoming too reliant on specific neurons. Instead, it encourages the network to learn more robust features by activating more diverse sets of neurons.\n",
    "\n",
    "<img src=\"./imgs/dropout.jpg\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Normalization\n",
    "\n",
    "Another technique that is used to keep our neural networks from memorizing data has to do with the range of the values that get passed between its inner layers.\n",
    "\n",
    "Input data coming into the network is most likely normalized, but after the first layer, the network weights might really change the distribution of the data as it flows through the network. Moreover, individual batches with different input value distributions can bias the network towards certain goals.\n",
    "\n",
    "<img src=\"./imgs/norm_activation.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization\n",
    "\n",
    "One way to handle these situations is to normalize the data as it passes through the network. Batch Normalization is the process of normalizing the activations of our network by using the mean and standard deviation of an activation neuron across a batch. The result is that the activations between batches become more similar. Batch normalization is dependent on batch size, so it's not effective for small batches.\n",
    "\n",
    "<img src=\"./imgs/norm_batch.jpg\" width=\"720px\"/>\n",
    "\n",
    "#### Layer Normalization\n",
    "\n",
    "Another form of inner-network normalization can be added to make sure no individual layer overpowers the network with activation values that are too large or too small.\n",
    "\n",
    "Layer Normalization scales activations using the mean and standard deviation of all activations across a layer. It's effective for sequence models like RNNs and Transformers, and for scenarios with small batch sizes, and doesn't require a large batch to get a good estimate for mean and standard deviation. \n",
    "\n",
    "<img src=\"./imgs/norm_layer.jpg\" width=\"720px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1] // 2),\n",
    "  nn.BatchNorm1d( x_train.shape[1] // 2),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(x_train.shape[1] // 2),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 2, x_train.shape[1] // 16),\n",
    "  nn.BatchNorm1d(x_train.shape[1] // 16),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(x_train.shape[1] // 16),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 16, x_train.shape[1] // 32),\n",
    "  nn.BatchNorm1d(x_train.shape[1] // 32),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(x_train.shape[1] // 32),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(x_train.shape[1] // 32, len(y_train.unique())),\n",
    ").to(mdevice)\n",
    "\n",
    "# lr := [1e-4, 1e-2]\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "    test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "    train_error = classification_error(train_labels, train_predictions)\n",
    "    test_error = classification_error(test_labels, test_predictions)\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}, train error: {train_error:.4f}, test error: {test_error:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "The train and test eval function diverged, but both keep decreasing, so this might be ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = get_labels(model, train_dataloader)\n",
    "test_labels, test_predictions = get_labels(model, test_dataloader)\n",
    "\n",
    "print(\"train error\", f\"{classification_error(train_labels, train_predictions):.4f}\")\n",
    "print(\"test error\", f\"{classification_error(test_labels, test_predictions):.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "Let's quickly review the model from last week.\n",
    "\n",
    "### Load Data\n",
    "\n",
    "The version of `LFWUtils.train_test_split()` in this week's utils class has an optional parameter `return_loader` that will return the data already in sensible `DataLoader` objects, so we don't have to repeat that code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.5, return_loader=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at Data\n",
    "\n",
    "Our `DataLoaders` are iterable objects, which means we need to do a bit of unpacking to get to actual labels and pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "print(LFWUtils.LABELS[label[0]])\n",
    "display(make_image(img[0], width=130))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Optimizer, Cost/Loss Function\n",
    "\n",
    "This is the model from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "image_length = LFWUtils.IMAGE_SIZE[0] * LFWUtils.IMAGE_SIZE[1]\n",
    "\n",
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length, image_length // 2),\n",
    "  nn.BatchNorm1d(image_length // 2),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 2),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 2, image_length // 16),\n",
    "  nn.BatchNorm1d(image_length // 16),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 16),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 16, image_length // 32),\n",
    "  nn.BatchNorm1d(image_length // 32),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 32),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 32, len(LFWUtils.LABELS)),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval\n",
    "\n",
    "Could've been in the loop, but we already know this model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Our model did ok, given the limited data that it has to learn from.\n",
    "\n",
    "Adding Dropout and Batch Normalization helped, but nothing we could do about having few images for some classes...\n",
    "\n",
    "Until now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Training Harder Again\n",
    "\n",
    "One technique for improving the training of a model is to add variability to the training data. This can be done with any kind of data, but is a little bit easier to see with images.\n",
    "\n",
    "### Image augmentation\n",
    "\n",
    "This is the process of adding similar, but different data to our training dataset. The new images are created by performing shape and color transformations on the original images, to get variations of the original shapes that we are trying to detect/classify.\n",
    "\n",
    "Intuitively, what we're trying to do is give the neural network more views of the same objects, and train it to be robust to shape distortions and color variations.\n",
    "\n",
    "A sideways, distorted and green Arnold Schwarzenegger is still an Arnold Schwarzenegger.\n",
    "\n",
    "<img src=\"./imgs/augmentation.jpg\" width=\"800px\"/>\n",
    "\n",
    "The `PyTorch` library provides a couple of handy transformation functions for us to add to our training dataset. A more complete list and examples cna be found [HERE](https://pytorch.org/vision/0.13/auto_examples/plot_transforms.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many Transforms with Composition\n",
    "\n",
    "Let's create a series of transforms and check what they do to the images.\n",
    "\n",
    "Some of these have a probability argument `p`, which can be used to specify how often the transformation should be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomRotation(degrees=15),\n",
    "    v2.RandomPerspective(distortion_scale=0.15, p=0.5),\n",
    "    # v2.RandomResizedCrop(size=(170, 130), scale=(.75, .9), antialias=True),\n",
    "    # v2.RandomAffine(degrees=15, translate=(0.1, 0.3), scale=(1.1, 1.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test on the first image of the first batch of our train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "print(LFWUtils.LABELS[label[0]])\n",
    "img0 = make_image(img[0], width=130)\n",
    "\n",
    "print(\"Original\")\n",
    "display(img0)\n",
    "\n",
    "print(\"Transformed\")\n",
    "timg0 = transforms(img0)\n",
    "display(timg0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment the DataLoader\n",
    "\n",
    "Since we have to perform this for images in the training dataset, and the `DataLoader` is responsible for giving us batches of randomly ordered images, it makes sense to delegate the transformations to the function that creates the `DataLoader`s.\n",
    "\n",
    "When it's time for our `DataSet` to grab an image, if there's a transformation function defined, and we're training (computing grads), we'll apply the transformation. Something like:\n",
    "\n",
    "```py\n",
    "if self.transform and torch.is_grad_enabled():\n",
    "  img = self.transform(img)\n",
    "```\n",
    "\n",
    "Now we can get our loader one more time, but this time, with image augmentation transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LFWUtils.train_test_split(0.3, return_loader=True, train_transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "print(LFWUtils.LABELS[label[0]])\n",
    "img0 = make_image(img[0], width=130)\n",
    "display(img0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Train\n",
    "\n",
    "Let's re-define our network, optimizer and loss function and see if this helps with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "image_length = LFWUtils.IMAGE_SIZE[0] * LFWUtils.IMAGE_SIZE[1]\n",
    "\n",
    "model =  nn.Sequential(\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length, image_length // 2),\n",
    "  nn.BatchNorm1d(image_length // 2),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 2),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 2, image_length // 16),\n",
    "  nn.BatchNorm1d(image_length // 16),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 16),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 16, image_length // 32),\n",
    "  nn.BatchNorm1d(image_length // 32),\n",
    "  nn.ReLU(),\n",
    "  # nn.LayerNorm(image_length // 32),\n",
    "\n",
    "  nn.Dropout(0.35),\n",
    "  nn.Linear(image_length // 32, len(LFWUtils.LABELS)),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 5e-3\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Cost vs. Eval Functions\n",
    "\n",
    "#### Why we need both\n",
    "\n",
    "Cost can go down without improving accuracy, precision or recall.\n",
    "\n",
    "Image we have $2$ samples, with the following correct labels:\n",
    "\n",
    "`[0, 1]`\n",
    "\n",
    "Out model computes the following probabilities for each label of each sample:\n",
    "\n",
    "`[0.1, 0.9], [0.9, 0.1]`\n",
    "\n",
    "which means it predicts `[1, 0]` for an accuracy of $0\\%$, and a cross entropy loss of $1.7$.\n",
    "\n",
    "After some amount of training, our model improves and now gives the following probabilities for the $2$ samples:\n",
    "\n",
    "`[0.45, 0.55], [0.51, 0.49]`\n",
    "\n",
    "making the predicted labels `[1, 0]`, so accuracy is still $0\\%$, while the cross entropy loss decreased to $0.7$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Information\n",
    "\n",
    "Our fully-connected layers do ok for this dataset, but they are not very efficient.\n",
    "\n",
    "There are $2$ main problems with using this approach to extract information about images:\n",
    "\n",
    "### Every pixel is connected to every other pixel\n",
    "\n",
    "Consider the first layer after the input layer: every neuron gets information about every pixel. This means that the content at the top-left corner of our image is connected to the content at the bottom-right corner, which is inefficient. We probably don't need our network to consider the entire content of the image at once in order to make decisions. It jumbles the pixel order and just makes the process harder. We might be better off telling our network to consider groups of neighboring pixels, since it's most likely for visual features to come from pixels that are near each other. In other words, we want to extract and preserve some kind of relative _Locality_ from our pixels.\n",
    "\n",
    "### Not all Arnolds are the same\n",
    "\n",
    "Let's say our network learned how to classify an Arnold Schwarzenegger that's closer to the left side of the image. If it wants to detect Arnolds on the right side of the image, or towards the top, it has to learn how to activate neurons that are associated with those sections of the image. This is also inefficient because it has to relearn to detect the same thing again, just because it's somewhere else in the image.\n",
    "\n",
    "Again, what we would like to do is group neighboring pixels, and have the groups go through similar neurons, so that any kind of learning can be applied independent of where shapes are located in the image. The technical name for this property is _Translation Invariance_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions\n",
    "\n",
    "We could try to come up with our own architecture and write some code for a neural network that doesn't fully connect our pixels, but rather considers neighboring regions of our image in groups of neurons.\n",
    "\n",
    "But, luckily, some maths combined with intuition from old-school systems and feature engineering can help us here.\n",
    "\n",
    "There's a type of mathematical operation called a convolution that combines $2$ arbitrary functions into a new function that basically has information about all the possible combinations of inputs for the $2$ original functions. The math definition looks like this:\n",
    "\n",
    "$\\displaystyle (f * g)(\\tau) = \\int_{-\\infty}^{\\infty}{f(\\tau)}g(1 - \\tau) d\\tau$\n",
    "\n",
    "For the practical, intuitive, definition of this operation when dealing with images, we'll make $f()$ be an image and $g()$ be different, but specific, combinations of numbers organized into $2D$ matrices called kernels.\n",
    "\n",
    "When we _convolve_ the image with the kernel, we calculate every possible overlap of our kernel with the image and, depending on the numbers we choose for the kernel, can extract different types of features from our pixels.\n",
    "\n",
    "<img src=\"./imgs/kernel_slide.jpg\" height=\"300px\"/>\n",
    "\n",
    "[SOME ANIMATIONS](https://hannibunny.github.io/mlbook/neuralnetworks/convolutionDemos.html)\n",
    "\n",
    "\n",
    "Classic image processing kernels for sharpening an image and extracting edges:\n",
    "\n",
    "<img src=\"./imgs/image_kernels.jpg\" height=\"300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about these kernels is that they operate on neighboring pixels by default, so they already take into account the _locality_ of the features they're trying to detect.\n",
    "\n",
    "We can now set up a neural network that is a collection of $2D$ image kernels, and let our training algorithm learn parameters for these kernels based on the training data. We don't have to specify that we want an edge-detection kernel, or a curved-shape kernel, or a horizontal blur kernel... the network will learn the kernels that it needs.\n",
    "\n",
    "And, since the same kernel slides over an entire image during convolution, once the network learns to extract lines on the left side of the image, it also knows how to extract lines on the right side of the image, or on top, or anywhere else. The parameters to the kernel are the same, they just get applied to different neighborhoods of pixels.\n",
    "\n",
    "If we combine our bank of kernels with another operation to reduce the size of our image as it moves through the network, we can create a type of dynamic filtering that detects whether certain features are present on our image.\n",
    "\n",
    "<img src=\"./imgs/cnn_layers.jpg\" height=\"320px\"/>\n",
    "\n",
    "Then, after we have reduced our feature maps to a small-enough shape we ca use fully-connected layers to finalize our classification.\n",
    "\n",
    "<img src=\"./imgs/cnn_fc.jpg\" height=\"320px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from data_utils import classification_error, display_confusion_matrix\n",
    "from image_utils import make_image\n",
    "\n",
    "from WK12_utils import LFWUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Network, New Loader\n",
    "\n",
    "Right now our loader returns lists of pixels for each image in a `tensor` of shape $(1, 22100)$. In order to use the convolution layers we actually need the images in a $2D$ tensor of shape $(170, 130)$.\n",
    "\n",
    "If we were working with color images, CNNs are even more particular, and instead of working with the original image shape of $H \\times W \\times C$ (`height` by `width` by `channel`), it wants a `tensor` with shape $C \\times W \\times H$.\n",
    "\n",
    "It makes sense to add some code to reshape our pixels inside our `FaceDataset` class:\n",
    "`pxs.reshape(-1, LFWUtils.IMAGE_SIZE[1], LFWUtils.IMAGE_SIZE[0])`\n",
    "\n",
    "Our `train_test_split()` function just has to know that we want to load our images for a `CNN` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = v2.Compose([\n",
    "\tv2.RandomHorizontalFlip(p=0.5),\n",
    "\tv2.RandomRotation(degrees=15),\n",
    "\tv2.RandomPerspective(distortion_scale=0.15, p=0.5),\n",
    "])\n",
    "\n",
    "train, test = LFWUtils.train_test_split(0.3, cnn_loader=True, train_transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if we're getting something sensible and if the transformations are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train))\n",
    "\n",
    "print(\"batch shape: B x C x H x W\")\n",
    "display(img.shape)\n",
    "\n",
    "print(\"img shape: C x H x W\")\n",
    "display(img[0].shape)\n",
    "\n",
    "display(v2.ToPILImage()(img[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Model\n",
    "\n",
    "This is how we define a convolution layer:\n",
    "\n",
    "`nn.Conv2d(Cin, Cout, kernel_size)`\n",
    "\n",
    "Where `Cin` is the number of input channels, `Cout` is output channels, and `kernel_size` the width of our kernel.\n",
    "\n",
    "We should still normalize the computations by batch, but this time using the $2D$ version of `BatchNorm()`, and after activation we perform the `MaxPool` operation, which takes the largest value in a $2 \\times 2$ region of our activations and condenses them into denser representation of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "width, height = LFWUtils.IMAGE_SIZE\n",
    "\n",
    "pool_width = (width - 2) // 2\n",
    "pool_height = (height - 2) // 2\n",
    "\n",
    "linear_length = pool_width * pool_height * 32\n",
    "\n",
    "model = nn.Sequential(\n",
    "  nn.Conv2d(1, 32, 3),\n",
    "  nn.BatchNorm2d(32),\n",
    "  nn.ReLU(),\n",
    "  nn.MaxPool2d(2),\n",
    "\n",
    "  # More Convs ?\n",
    "\n",
    "  nn.Flatten(),\n",
    "\n",
    "  nn.Linear(linear_length, 512),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Linear(512, len(LFWUtils.LABELS)),\n",
    ").to(mdevice)\n",
    "\n",
    "learning_rate = 2e-2\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "out = model(img)\n",
    "\n",
    "print(\"Input shape:\", img.shape)\n",
    "print(\"Output shape:\", out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(32):\n",
    "  model.train()\n",
    "  for imgs, labels in train_dataloader:\n",
    "    optim.zero_grad()\n",
    "    labels_pred = model(imgs)\n",
    "    loss = loss_fn(labels_pred, labels)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "  if e % 4 == 3:\n",
    "    print(f\"Epoch: {e} loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels, train_predictions = LFWUtils.get_labels(model, train)\n",
    "test_labels, test_predictions = LFWUtils.get_labels(model, test)\n",
    "train_error = classification_error(train_labels, train_predictions)\n",
    "test_error = classification_error(test_labels, test_predictions)\n",
    "print(f\"train error: {train_error:.4f}, test error: {test_error:.4f}\")\n",
    "\n",
    "display_confusion_matrix(train_labels, train_predictions, display_labels=LFWUtils.LABELS)\n",
    "display_confusion_matrix(test_labels, test_predictions, display_labels=LFWUtils.LABELS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
